{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea94702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "from baukit import Widget, Property, Trigger, show\n",
    "\n",
    "import torch, numpy as np\n",
    "import os, re, json\n",
    "from matplotlib import cm, pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Include prompt creation helper functions\n",
    "from utils.prompt_utils import *\n",
    "from attentionVisualizationWidget import TokenVizWidget, AttnHeadSelectorWidget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933401e",
   "metadata": {},
   "source": [
    "# Notes on Using the TokenVizWidget\n",
    "\n",
    "Usage:\n",
    "- Use the selectors to pick an attention head. Hover over a token to display its attention pattern and tooltip.\n",
    "- Click on a token to lock its attention pattern. Click the same token to unlock. \n",
    "- If no token is hover-selected or clicked, the default display is the contents of the `default_matrix` array.\n",
    "\n",
    "Logistics:\n",
    "- For this example, we have attention data from GPT-J. The model has 28 layers with 16 heads per layer.<br>\n",
    "- There are two widgets the attn head selector widget and the token viz widget. <br>\n",
    "- The `TokenVizWidget` requires the tokenized prompt text, `attention matrix` of size `(n_layers,n_heads,n_tokens,n_tokens)`, and a `default matrix` of size `(n_layers,n_heads,n_tokens)` to build.<br> \n",
    "- Unless you have a specific default you're interested in, you can just create an `ndarray` of 0's to fill the slot.\n",
    "\n",
    "- We hook up the two widgets using `current_layer = ahw.prop('current_layer'),current_head=ahw.prop('current_head')`, and `tvw.on('current_layer', update_attn_matrix), tvw.on('current_head', update_attn_matrix)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6875800",
   "metadata": {},
   "source": [
    "# Google Review Restaurant Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = r\"EleutherAI/gpt-j-6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "# Load pre-computed Attention & Prompt Data\n",
    "attn_matrix = np.load(f\"data/restaurant_attention.npy\")\n",
    "prompt_data = json.load(open('data/restaurant_prompt.json', 'r'))\n",
    "\n",
    "prompt_text = create_prompt(f\"If you're in the mood for some quick fast food, Wendy's is always a solid choice.\", prompt_data=prompt_data)\n",
    "token_ids = tokenizer(prompt_text)['input_ids']\n",
    "text_tokens = [repr(tokenizer.decode(x))[1:-1] for x in token_ids]\n",
    "\n",
    "# Init config vars\n",
    "N_TOKENS = len(text_tokens)\n",
    "N_LAYERS = 28\n",
    "N_HEADS = 16\n",
    "\n",
    "# Load Default Matrix\n",
    "default_matrix = np.zeros((N_LAYERS,N_HEADS,N_TOKENS))\n",
    "\n",
    "def update_attn_matrix():\n",
    "    l,h = ahw.prop('current_layer').value, ahw.prop('current_head').value\n",
    "    new_attn_matrix = attn_matrix[l,h]\n",
    "    tvw.token_attn = new_attn_matrix.tolist()\n",
    "    # Update Default Matrix & Corresponding Color Matrix\n",
    "    update_default_matrix()        \n",
    "\n",
    "def update_default_matrix():        \n",
    "    l,h = ahw.prop('current_layer').value, ahw.prop('current_head').value\n",
    "    new_default_matrix = default_matrix[l,h,:]\n",
    "    tvw.colors_matrix = tvw.color_sample(new_default_matrix, cm.bwr)\n",
    "    tvw.default_display = new_default_matrix.tolist()   \n",
    "    \n",
    "    \n",
    "# Initialize Widgets\n",
    "# head_groupings = {'repeat_token':[(6,2), (3,9), (8,7)],'induction':[(4,0),(8,1),(16,7)], 'prev_token':[(2,11),(3,5)], 'prev_answer':[(13,13), (13,2)]}\n",
    "ahw = AttnHeadSelectorWidget(n_layers=N_LAYERS,n_heads=N_HEADS)\n",
    "tvw = TokenVizWidget(text = text_tokens, token_attn=attn_matrix[0,0].tolist(), \n",
    "                     default_display=default_matrix[0,0].tolist(), \n",
    "                     current_layer = ahw.prop('current_layer'),current_head=ahw.prop('current_head'))\n",
    "\n",
    "# Setup python-side listeners to update attn matrix & token dependence matrix\n",
    "tvw.on('current_layer', update_attn_matrix)\n",
    "tvw.on('current_head', update_attn_matrix)\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Widgets\n",
    "show([ahw,tvw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c684fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
